{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project summary :**\n",
    "\n",
    "**I. Data exploration**\n",
    "\n",
    "**II. Data pre-processing**\n",
    "\n",
    "**III. Model building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "# classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# pre-processing\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I. Data exploration**\n",
    "\n",
    "First of all let us take a look at our data to try to detect challenges that we might encounter. In the dataset description we know that it is heavily unbalanced so we should definitly explore that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data =  pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds 99.83 % of the dataset\n",
      "Frauds 0.17 % of the dataset\n"
     ]
    }
   ],
   "source": [
    "print('No Frauds', round(data['Class'].value_counts()[0]/len(data) * 100,2), '% of the dataset')\n",
    "print('Frauds', round(data['Class'].value_counts()[1]/len(data) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Class Distributions \\n (0: No Fraud || 1: Fraud)')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = [\"#0101DF\", \"#DF0101\"]\n",
    "\n",
    "sns.countplot('Class', data=data, palette=colors)\n",
    "plt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see that 99,83% of the data is composed of valid transactions as opposed to 0,17% of fraudulous transactions. This makes sense since for any given bank you'd better hope that most transactions are not frauds. However, this might prove to be a challenge while bulding our classification model since a model that **systematically** predicts valid transactions would be right 99,83% of the time. Yet, it would be useless since it is these rare cases of fraud that we want to catch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ammount of null values : 0\n"
     ]
    }
   ],
   "source": [
    "print(\"ammount of null values :\", data.isnull().sum().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above we do not have any null values nor do we have non numerical values. This will make it easier for us going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**II. Data pre-processing**\n",
    "\n",
    "The main issue we are going to challenge when pre-processing our data is to make our data set balanced again, so that our model can learn to distinguish between fraudulent and valid transactions. There are different methods to do this. You can augment the smaller class or disminsih the bigger one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraudulent transactions count : 492\n",
      "valid transactions count : 492\n"
     ]
    }
   ],
   "source": [
    "valid_transactions = data.loc[data.Class == 0]\n",
    "fraudulent_transactions = data.loc[data.Class == 1]\n",
    "\n",
    "subset_valid_transactions = valid_transactions.sample(n=492)\n",
    "balanced_data = (pd.concat([fraudulent_transactions, subset_valid_transactions])).sample(frac=1, axis=0).reset_index(drop=True)\n",
    "print(\"fraudulent transactions count :\", len(balanced_data.loc[balanced_data.Class == 1]))\n",
    "print(\"valid transactions count :\", len(balanced_data.loc[balanced_data.Class == 0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just extarcted a random sample of length = 492 from the valid transactions and concatenated it with the fraudulent transactions to compose a balanced dataset. Now, before we proceed to bulding ou model, let's scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data['scaled_amount'] = StandardScaler().fit_transform(balanced_data['Amount'].values.reshape(-1,1))\n",
    "balanced_data['scaled_time'] = StandardScaler().fit_transform(balanced_data['Time'].values.reshape(-1,1))\n",
    "\n",
    "balanced_data.drop(['Time','Amount'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III. Model building**\n",
    "\n",
    "Our goal is to be able to detect fraudulent transactions with the highest recall ? accuracy ? precision ? possibe. To do so, we will try a couple of different classifiers. Let us first split our data into training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = balanced_data.drop('Class', 1)\n",
    "y = balanced_data['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/younes/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/younes/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/younes/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/younes/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/younes/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/younes/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/younes/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/younes/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/younes/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/younes/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/younes/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/younes/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/younes/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/younes/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/younes/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/younes/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log reg best f1 score : 0.9508508027257353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/younes/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/younes/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/younes/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "search_logistic_regression = GridSearchCV(LogisticRegression(), log_reg_params, cv=5, scoring='f1')\n",
    "search_logistic_regression.fit(X_train, y_train)\n",
    "print(\"log reg best f1 score :\", search_logistic_regression.best_score_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kn best f1 score : 0.9428362175656465\n"
     ]
    }
   ],
   "source": [
    "knears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "search_k_neighbours = GridSearchCV(KNeighborsClassifier(), knears_params, scoring='f1')\n",
    "search_k_neighbours.fit(X_train, y_train)\n",
    "print(\"kn best f1 score :\", search_k_neighbours.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc best f1 score : 0.9507960634507733\n"
     ]
    }
   ],
   "source": [
    "svc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n",
    "search_SVC = GridSearchCV(SVC(), svc_params, cv=5, scoring='f1')\n",
    "search_SVC.fit(X_train, y_train)\n",
    "print(\"svc best f1 score :\", search_SVC.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree best f1 score : 0.9152510806412856\n"
     ]
    }
   ],
   "source": [
    "decision_tree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \"min_samples_leaf\": list(range(5,7,1))}\n",
    "search_decision_tree = GridSearchCV(DecisionTreeClassifier(), decision_tree_params, cv=5, scoring='f1')\n",
    "search_decision_tree.fit(X_train, y_train)\n",
    "print(\"decision tree best f1 score :\", search_decision_tree.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like our best classifier on this problem is logisitic regrssion. Now we want to check if we are overfiting the training data by using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log reg f1 score : 0.9282296650717703\n"
     ]
    }
   ],
   "source": [
    "log_reg = search_decision_tree.best_estimator_\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print(\"log reg f1 score :\", f1_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like our model can generalize since their performance on the test set is similar to that on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
